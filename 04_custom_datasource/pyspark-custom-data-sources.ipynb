{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cefa018f-146a-4c4b-843f-c762f3f4c3c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PySpark custom data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bc9dabe-2d05-4059-9a85-5dc239cca8a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "[DataBricks docs](https://docs.databricks.com/aws/en/pyspark/datasources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48f0a775-aab1-49b8-8da2-4ff92c95f81d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "PySpark Custom Data Sources คือ feature ที่ช่วยให้เราสามารถอ่านข้อมูลจากแหล่งข้อมูลแบบ custom ได้เอง และยังสามารถเขียนไปยังปลายทางได้แบบ custom เองใน Apache Spark ได้ โดยใช้ Python\n",
    "\n",
    "ซึ่งในไฟล์นี้ จะมีด้วยกัน 3 ตัวอย่าง (อ้างอิงตาม DataBricks docs) ดังนี้\n",
    "- Batch Query\n",
    "- GitHub DataSource\n",
    "- Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b607616c-ff3f-4db6-a726-9868f9091fc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Implement the data source subclass\n",
    "\n",
    "| Property / Method | Description |\n",
    "|------------------------------------|---------------------------------------------------------------------------|\n",
    "| **name** | **Required** เป็นการระบุชื่อของ Data Source |\n",
    "| **schema** | **Required** เป็นการระบุโครงสร้างข้อมูล (Schema) ของ Data Source ที่จะอ่านหรือเขียน |\n",
    "| reader() | ต้องคืนค่า `DataSourceReader` เพื่อให้ Data Source อ่านข้อมูลแบบ Batch |\n",
    "| writer() | ต้องคืนค่า `DataSourceWriter` เพื่อให้ Data Sink เขียนข้อมูลแบบ Batch |\n",
    "| streamReader() / simpleStreamReader() | ต้องคืนค่า `DataSourceStreamReader` เพื่อให้ Data Stream อ่านข้อมูลแบบ Streaming |\n",
    "| streamWriter() | ต้องคืนค่า `DataSourceStreamWriter` เพื่อให้ Data Stream เขียนข้อมูลแบบ Streaming |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07b6a641-db7e-470f-bb9c-b55074b5b120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Example 1: Create a PySpark DataSource for batch query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d783e768-c3f0-4a4c-b20c-ad1aa97437b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ติดตั้ง Library ที่จำเป็น ในที่นี้ เราจะใช้ faker มาช่วยในการจำลองข้อมูลตัวอย่างกัน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d55a1b1f-5860-46e9-8e56-8a2d129f0d03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7c78e46-f1c7-4f12-8e2b-ed8d77661f7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "หลังจาก install จะมี Note บอกให้ restart kernel โดยใช้ `%restart_python` หรือ `dbutils.library.restartPython()`\n",
    "\n",
    "(ในที่นี้เลยเลือกใช้ `%restart_python`) แล้วกดรันใหม่อีกรอบ จะขึ้นแจ้งเตือนเหมือนเดิม แต่สามารถใช้งาน faker lib ได้แล้ว"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d498775-b946-46c9-8109-5766bd5cd199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4db210f-baaf-48b0-9ed8-b1994aae4434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Define the example DataSource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cba68bd-7a7a-4ec5-8bb3-7718562513cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ขั้นแรก : Define Class หลักสำหรับแหล่งข้อมูลขึ้นมาก่อน โดยจะมี 3 ส่วนใน Class คือ\n",
    "- name : ชื่อที่จะใช้ตอนเราเอามาใช้กับ spark\n",
    "- schema : โครงสร้างข้อมูลที่จะอ่าน ว่ามี column อะไรบ้าง, data type เป็นยังไง \n",
    "- reader : วิธีการส่งต่องานไปยังตัวที่ใช้ในการอ่านข้อมูลจริง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffebaad5-9725-4269-b82b-8ca6a85ffde1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.datasource import DataSource, DataSourceReader\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "class FakeDataSource(DataSource):\n",
    "    \"\"\"\n",
    "    An example data source for batch query using the `faker` library.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def name(cls):\n",
    "        return \"fake\"\n",
    "\n",
    "    def schema(self):\n",
    "        return \"name string, date string, zipcode string, state string\"\n",
    "\n",
    "    def reader(self, schema: StructType):\n",
    "        return FakeDataSourceReader(schema, self.options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4011b73f-654c-4539-89d5-1d0aca15ef73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Implement the reader for a batch query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95c0fc38-3f3d-4435-a29c-6a19a1e92cce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ขั้นที่ 2 : สร้างตัวอ่านข้อมูล\n",
    "\n",
    "Method ที่สำคัญคือ `read()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "722f6842-b2d0-4d73-a84e-37f73b836f3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class FakeDataSourceReader(DataSourceReader):\n",
    "\n",
    "    def __init__(self, schema, options):\n",
    "        self.schema: StructType = schema\n",
    "        self.options = options\n",
    "\n",
    "    def read(self, partition):\n",
    "        # Library imports must be within the method.\n",
    "        from faker import Faker\n",
    "        fake = Faker()\n",
    "\n",
    "        # Every value in this `self.options` dictionary is a string.\n",
    "        num_rows = int(self.options.get(\"numRows\", 3))\n",
    "        for _ in range(num_rows):\n",
    "            row = []\n",
    "            for field in self.schema.fields:\n",
    "                value = getattr(fake, field.name)()\n",
    "                row.append(value)\n",
    "            yield tuple(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b68e5179-5b28-43af-b435-788518f2825e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "จากโค้ด ใน method `read()` มีจุดให้สังเกตเพิ่มเติม 2 ส่วน\n",
    "1. ข้อมูลตัวอย่างถูกสร้างจาก lib faker ที่เราติดตั้งเมื่อก่อนหน้านี้นั่นเอง\n",
    "2. เราจะเห็นว่า method นี้ใช้คำสั่ง `yield` ในการส่งข้อมูลกลับออกมา ซึ่งคำสั่งนี้มันจะทำการส่งค่าออกมาทีละค่า แล้ว pause ไว้ พอถูกเรียกครั้งต่อไป มันก็จะทำงานต่อจากจุดเดิม ซึ่งจะมีผลดีเมื่อเจอข้อมูลที่มันใหญ่มากๆ (อันนี้สรุปเท่าที่เข้าใจในแง่ของ python จะมีกล่าวถึงเพิ่มเติมในตัวอย่างหลังๆ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d7d2618-022a-421e-b2e2-7590bd046675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Register and use the example data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55ae00cb-eafb-47f9-94fc-5be350563788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ขั้นสุดท้ายสำหรับ e.g. นี้ : การ register & use\n",
    "\n",
    "- พอสร้าง class ที่จำเป็นในขั้น 1 & 2 เสร็จแล้ว ก็ต้องบอกให้ spark รู้ก่อน โดยใช้คำสั่ง `register`\n",
    "- เรียกใช้งาน โดยใช้ name ที่เราตั้งไว้ เติมใน `.format()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4cb9995-7e95-41dd-ae5a-33a566ed194f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.dataSource.register(FakeDataSource)\n",
    "spark.read.format(\"fake\").load().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62942485-f4d8-464c-a358-1716c5ef9867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ถ้าต้องการเพียงแค่ 2 คอลัมน์ก็ทำได้ ให้ระบุ schema ได้เลย\n",
    "spark.read.format(\"fake\").schema(\"name string, company string\").load().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b03e5424-7f04-4524-aeb5-279a4bd4fdbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ถ้าต้องการจำนวนแถวมากกว่า 3 สามารถระบุได้ด้วย\n",
    "spark.read.format(\"fake\").option(\"numRows\", 5).load().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7be9f554-8dbc-445a-a901-d8698979b24e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Example 2: Create a PySpark GitHub DataSource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a2df685-595b-4fb7-bb85-643aac5193e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ในส่วนนี้ เป็นการดึงข้อมูลจาก GitHub เพื่อมาแสดงเป็นตารางใน spark\n",
    "- ***เดิม*** ตาม docs จะมีการนำเข้าข้อมูลแบบ [**Variant**](https://docs.databricks.com/aws/en/sql/language-manual/data-types/variant-type) เข้ามาโดยใช้ python ด้วย แต่จากการทดลองหลายครั้ง พบว่าติดปัญหาผ่านการใช้ python และยังหาวิธีแก้ไขไม่สำเร็จ \n",
    "- หากใช้ SQL ปกติอาจจะนำเสนอเกี่ยวกับรายละเอียดของ Variant Data ได้ ดังตัวอย่างด้านล่าง ผลลัพธ์จะแสดงให้เห็นว่า ตรงส่วน col `my_variant_data` เป็น object ไม่ใช่ str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e43e4b39-a0d6-49aa-ae71-a4e4faa3c6bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  parse_json('{\"name\": \"Variant\", \"stamina\": 100, \"active\": true}') AS my_variant_data,\n",
    "  parse_json('{\"name\": \"Variant\", \"stamina\": 100}'):stamina AS extracted_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "382319d4-44bd-4133-b1e7-8eeea9da2218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ซึ่งเดิมที การจะใช้ข้อมูลประเภท Variant ได้ จะต้องใช้ runtime version 17.1 ขึ้นไป และสิ่งที่เรามีควรจะสามารถใช้งานได้บน python ด้วย ทั้งนี้ เพื่อให้ตัวอย่างนี้แสดงเนื้อหาในการนำข้อมูลจากแหล่งต่างๆแบบกำหนดเองมาใช้งานต่อ จึงจะปรับประเภท data ใน schema ให้เป็นประเภทแบบที่เราๆรู้จักกันก่อน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "151250bc-3051-4055-9604-5da9cb7376ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT current_version();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a134f7f8-d505-4740-bff3-add20b5b0fb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Define the GitHub DataSource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff936f0a-d250-488d-9daa-6f6a52713b25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ขั้นแรก : Define Class หลักสำหรับแหล่งข้อมูลขึ้นมา โดยจะมี 3 ส่วนใน Class คือ\n",
    "- name : ชื่อที่จะใช้ตอนเราเอามาใช้กับ spark\n",
    "- schema : โครงสร้างข้อมูลที่จะอ่าน ว่ามี column อะไรบ้าง, data type เป็นยังไง \n",
    "- reader : วิธีการส่งต่องานไปยังตัวที่ใช้ในการอ่านข้อมูลจริง\n",
    "\n",
    "ซึ่งส่วนนี้จะเหมือนกันกับตัวอย่างแรก"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c756da-24ae-4407-a86a-7a28c30d2eeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.datasource import DataSource, DataSourceReader\n",
    "from pyspark.sql.types import VariantVal\n",
    "\n",
    "class GithubVariantDataSource(DataSource):\n",
    "    @classmethod\n",
    "    def name(self):\n",
    "        return \"githubVariant\"\n",
    "    def schema(self):\n",
    "        return \"id int, title string, user string, created_at string, updated_at string\"\n",
    "    def reader(self, schema):\n",
    "        return GithubVariantPullRequestReader(self.options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6baf3458-36ac-43db-af9b-8b5c1ac1725e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Implement the reader to retrieve pull requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cb0375f-3c8e-4cc3-8a19-68f7bd58f2ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ขั้นที่ 2 : ส่วนนี้คือส่วนของการสร้างตัวอ่านข้อมูล แต่จะมีความซับซ้อนกว่าตัวอย่างแรก เนื่องจากเป็นการดึงข้อมูลจาก GitHub\n",
    "\n",
    "มี 2 ส่วน คือ\n",
    "1. `__init__` ส่วนนี้จะเป็นส่วนที่รับ path ของ repo, token(ถ้ามี) หาก repo ว่าง จะไม่สามารถทำงานต่อได้\n",
    "2. `read` ส่วนนี้จะมีการทำงานย่อยลงไปอีก กล่าวคือ\n",
    "> - เตรียม HTTP Header\n",
    "> - Request & Response\n",
    "> - แปลงเป็น row ส่งกลับ (yield rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9ede229-d905-41a2-87f6-73090523b57c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class GithubVariantPullRequestReader(DataSourceReader):\n",
    "    def __init__(self, options):\n",
    "        self.token = options.get(\"token\")\n",
    "        self.repo = options.get(\"path\")\n",
    "        if self.repo is None:\n",
    "            raise Exception(f\"Must specify a repo in `.load()` method.\")\n",
    "\n",
    "    def read(self, partition):\n",
    "        header = {\n",
    "            \"Accept\": \"application/vnd.github+json\", # ต้องการข้อมูลแบบ JSON\n",
    "        }\n",
    "        if self.token is not None: # ถ้ามี token ให้แนบไปด้วย\n",
    "            header[\"Authorization\"] = f\"Bearer {self.token}\"\n",
    "        url = f\"https://api.github.com/repos/{self.repo}/pulls\" # เตรียม address ปลายทาง\n",
    "        response = requests.get(url, headers=header) # สร้างตัวแปร response เพื่อไปรับผลจากการ request\n",
    "        response.raise_for_status() # ตรวจสอบ status\n",
    "        prs = response.json() # แปลงเป็น json\n",
    "        for pr in prs: # loop ส่งข้อมูลกลับออกมาเป็น row\n",
    "            yield Row(\n",
    "                id = pr.get(\"number\"),\n",
    "                title = pr.get(\"title\"),\n",
    "                user = pr.get(\"user\"),\n",
    "                created_at = pr.get(\"created_at\"),\n",
    "                updated_at = pr.get(\"updated_at\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6653402-fd46-48d7-8ca9-5a49cf4d73d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Register and use the data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d79c80ba-46ca-48b4-8d45-7a6156ebf89a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "เหมือนเดิมจากที่เราเคยทำในตัวอย่างแรก เมื่อเรา custom data source ขึ้นมา ต้องมีการ `register` ให้ spark ทราบ แล้วใช้งานต่อไปได้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d3c3b4b-44d2-49f1-ab97-2a8f6bf11ed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.dataSource.register(GithubVariantDataSource)\n",
    "spark.read.format(\"githubVariant\").option(\"numRows\", 3).load(\"apache/spark\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b000069-e028-445c-af17-4e4f76ddc674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Example 3: Create PySpark DataSource for streaming read and write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6790acd8-8475-4beb-ba40-b415681bdcdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ตัวอย่างนี้จะเป็นเชิง concept เท่านั้น เนื่องจากข้อจำกัดของ free version บน databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61a0eed2-2ee9-4c76-b1b6-05c9617af6f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Define the example DataSource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b68a16d-8373-48e1-92d7-ee70b8eeecf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ขั้นแรก : Define Class หลักสำหรับแหล่งข้อมูลขึ้นมา โดยจะมี 4 ส่วนใน Class คือ\n",
    "- name : ชื่อที่จะใช้ตอนเราเอามาใช้กับ spark\n",
    "- schema : โครงสร้างข้อมูล ว่ามี column อะไรบ้าง, data type เป็นยังไง\n",
    "- streamReader : อ่านข้อมูลแบบ Streaming\n",
    "- streamWriter : เขียนข้อมูลแบบ Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bea8bc17-c7d8-4dcd-977d-42d93d3de8b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.datasource import DataSource, DataSourceStreamReader, SimpleDataSourceStreamReader, DataSourceStreamWriter\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "class FakeStreamDataSource(DataSource):\n",
    "    \"\"\"\n",
    "    An example data source for streaming read and write using the `faker` library.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def name(cls):\n",
    "        return \"fakestream\"\n",
    "\n",
    "    def schema(self):\n",
    "        return \"name string, state string\"\n",
    "\n",
    "    def streamReader(self, schema: StructType):\n",
    "        return FakeStreamReader(schema, self.options)\n",
    "\n",
    "    # If you don't need partitioning, you can implement the simpleStreamReader method instead of streamReader.\n",
    "    # def simpleStreamReader(self, schema: StructType):\n",
    "    #    return SimpleStreamReader()\n",
    "\n",
    "    def streamWriter(self, schema: StructType, overwrite: bool):\n",
    "        return FakeStreamWriter(self.options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72e3fa42-e417-495e-a03d-00d2158bf458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Implement the stream reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5156c613-0954-439b-bccd-3429c6e4bfc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "จาก step แรก จะมีส่วนของ comment ที่บอกว่า\n",
    "> If you don't need partitioning, you can implement the `simpleStreamReader` method instead of `streamReader`.\n",
    "\n",
    "ซึ่งจะเกี่ยวกับประเภทข้อมูลที่เราจะอ่านด้วย ดังนั้น ใน step นี้จะมีตัวอย่างของทั้ง 2 แบบ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f878908-bb3f-4933-8858-f8061f6ff153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DataSourceStreamReader implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "836f43f7-f89f-4132-b743-65003d04cc03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ในส่วนของตัวอย่างนี้ จะเหมาะกับข้อมูลที่มีขนาดใหญ่ มีการแบ่ง partition ในการอ่านข้อมูล และมีการรับ partition เป็น input โดยจะแบ่งออกเป็น 2 part คือ\n",
    "1. RangePartition ส่วนแบ่งงาน ซึ่งจะช่วยอำนวยความสะดวกให้ executor หลายตัวช่วยกันอ่านกรณีข้อมูลมีจำนวนมาก\n",
    "2. FakeStreamReader ส่วนอ่านข้อมูล จะมีรายละเอียดปลีกย่อยในแต่ละ method เพิ่มเติม ดังนี้\n",
    "- `initialOffset` จุดเริ่มต้นของ stream , เป็น dict โดยเริ่มจาก 0\n",
    "- `latestOffset` จุดล่าสุดของข้อมูล จะเรียกทุกครั้งที่มีการ trigger ซึ่งจะบอกว่าข้อมูลล่าสุดถึงไหนแล้ว โดยในตัวอย่างนี้ จะเป็นการจำลองว่า ข้อมูลที่เข้ามาใหม่เพิ่มขึ้นทีละ 2 หน่วย นั่นแปลว่า offset ใหม่ก็จะเพิ่มขึ้นไปเรื่อยๆ\n",
    "- `partitions` วางแผนกระจายงาน โดยรับค่า start offset, end offset และคืนค่าออกมาเป็น**ลำดับ**ของ `InputPartition` แต่ในกรณีเฉพาะตัวอย่างนี้ จะเป็นการคืนค่าออกมาเพียง `InputPartition` เดียว สังเกตจากลำดับที่มีเพียงลำดับเดียว\n",
    "- `commit` แจ้งสถานะว่าสำเร็จ, clean up resources\n",
    "- `read` ขั้นนี้คือตัวที่อ่านข้อมูลจริง โดยจะรับ partition ที่เข้ามาจากขั้นตอน partitions ก่อนหน้านี้ และส่ง record กลับไปแบบ Iterator ซึ่งภายใน Iterator จะเป็น tuple โดยแต่ละ tuple = 1 row \n",
    "> สังเกตได้ว่า ในขั้นนี้ เราไม่ได้ใช้ return แต่ใช้ `yield` ซึ่งคุณสมบัติของ yield จะเป็นดังตัวอย่างแรกๆที่เราเคยกล่าวไปในเชิงของ python พอกล่าวมาถึงจุดนี้ จะมีส่วนที่เรียกว่า `Iterator` แล้ว\n",
    "> - ดังนั้น `yield` จะใช้ในการคืนข้อมูลออกมาเป็น Iterator ซึ่ง executor สามารถอ่านและส่ง record ออกมาทีละ row โดยไม่ต้อง load ข้อมูลทั้งหมดของ partition ขึ้นมาใน memory พร้อมกัน จึงเหมาะกับข้อมูลที่มีขนาดใหญ่มากๆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1ad78c7-20c1-4141-ad75-c1adb1cf80cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.datasource import InputPartition\n",
    "from typing import Iterator, Tuple\n",
    "import os\n",
    "import json\n",
    "\n",
    "# 1 ส่วนแบ่งงาน\n",
    "class RangePartition(InputPartition):\n",
    "    def __init__(self, start, end):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "# 2 ส่วนอ่านข้อมูล\n",
    "class FakeStreamReader(DataSourceStreamReader):\n",
    "    def __init__(self, schema, options):\n",
    "        self.current = 0 # offset ปัจจุบัน\n",
    "\n",
    "    def initialOffset(self) -> dict:\n",
    "        \"\"\"\n",
    "        Returns the initial start offset of the reader.\n",
    "        \"\"\"\n",
    "        return {\"offset\": 0}\n",
    "\n",
    "    def latestOffset(self) -> dict:\n",
    "        \"\"\"\n",
    "        Returns the current latest offset that the next microbatch will read to.\n",
    "        \"\"\"\n",
    "        self.current += 2\n",
    "        return {\"offset\": self.current}\n",
    "\n",
    "    def partitions(self, start: dict, end: dict):\n",
    "        \"\"\"\n",
    "        Plans the partitioning of the current microbatch defined by start and end offset. It\n",
    "        needs to return a sequence of :class:`InputPartition` objects.\n",
    "        \"\"\"\n",
    "        return [RangePartition(start[\"offset\"], end[\"offset\"])]\n",
    "\n",
    "    def commit(self, end: dict):\n",
    "        \"\"\"\n",
    "        This is invoked when the query has finished processing data before end offset. This\n",
    "        can be used to clean up the resource.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def read(self, partition) -> Iterator[Tuple]:\n",
    "        \"\"\"\n",
    "        Takes a partition as an input and reads an iterator of tuples from the data source.\n",
    "        \"\"\"\n",
    "        start, end = partition.start, partition.end\n",
    "        for i in range(start, end):\n",
    "            yield (i, str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10975250-e634-4f23-aab6-6e3ed0b59e9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SimpleDataSourceStreamReader implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbeb90c0-be47-4bab-9b62-f908758fb566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ในส่วนของตัวอย่างนี้ จะเหมาะกับข้อมูลที่ไม่ได้มีขนาดใหญ่มาก เพราะ**ไม่ได้มีการแบ่ง** partition ในการอ่านข้อมูล\n",
    "\n",
    "โดยจะมี method ภายใน ดังนี้\n",
    "- `initialOffset` กำหนดจุดเริ่มต้น\n",
    "- `read` รับค่า start offset เข้ามา แล้วคืนค่า Iterator, จุด start offset ถัดไป โดยในตัวอย่างคือการอ่านข้อมูลเพิ่ม 2 แถวจากจุดที่ start และคำนวณ start offset ถัดไป\n",
    "- `readBetweenOffsets` ส่วนนี้มีไว้ในกรณีที่เกิด failed ซึ่งจะรับ offset ทั้ง start และ end แล้วอ่านข้อมูลตรง offset ช่วงนั้น\n",
    "- `commit` หน้าที่แบบเดียวกันกับตัวอย่างก่อนหน้า"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e41f0bf8-b0eb-44d3-988e-09588a66045c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SimpleStreamReader(SimpleDataSourceStreamReader):\n",
    "    def initialOffset(self):\n",
    "        \"\"\"\n",
    "        Returns the initial start offset of the reader.\n",
    "        \"\"\"\n",
    "        return {\"offset\": 0}\n",
    "\n",
    "    def read(self, start: dict) -> (Iterator[Tuple], dict):\n",
    "        \"\"\"\n",
    "        Takes start offset as an input, then returns an iterator of tuples and the start offset of the next read.\n",
    "        \"\"\"\n",
    "        start_idx = start[\"offset\"]\n",
    "        it = iter([(i,) for i in range(start_idx, start_idx + 2)])\n",
    "        return (it, {\"offset\": start_idx + 2})\n",
    "\n",
    "    def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[Tuple]:\n",
    "        \"\"\"\n",
    "        Takes start and end offset as inputs, then reads an iterator of data deterministically.\n",
    "        This is called when the query replays batches during restart or after a failure.\n",
    "        \"\"\"\n",
    "        start_idx = start[\"offset\"]\n",
    "        end_idx = end[\"offset\"]\n",
    "        return iter([(i,) for i in range(start_idx, end_idx)])\n",
    "\n",
    "    def commit(self, end):\n",
    "        \"\"\"\n",
    "        This is invoked when the query has finished processing data before end offset. This can be used to clean up resources.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "420360f1-6ef3-4a17-aa14-2d5185dd6cc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Implement the stream writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93bff9b2-d9f1-4dae-afdb-df6fb8a26f60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "มาสู่ขั้นตอนของการเขียนกันบ้าง ในส่วนนี้มี 2 class คือ\n",
    "1. SimpleCommitMessage\n",
    "2. FakeStreamWriter\n",
    "\n",
    "---\n",
    "**SimpleCommitMessage**\n",
    "- คลาสนี้ไว้ใช้สำหรับการรายงานผลจาก executor\n",
    "\n",
    "---\n",
    "**FakeStreamWriter**\n",
    "\n",
    "- คลาสนี้ไว้ใช้สำหรับการเขียนหลักๆ มี method 3 ส่วน คือ\n",
    "    - `write` : ทำงานบน executor ซึ่งจะรับข้อมูลจริงมาประมวลผล และข้อมูลที่รับมาจะเป็น iterator หลังจากเขียนเสร็จก็จะมี commit message กลับมา\n",
    "    - `commit` : รับ commit message จากทุก executor ที่ทำงานสำเร็จ จนครบ แล้วตัดสินใจว่าจะทำอะไรกับมันต่อไป ซึ่งในที่นี้ เราเขียน metadata ของแต่ละ micro-batch (จำนวนแถว และ จำนวน partition) ลงใน json file\n",
    "    - `abort` : ในกรณีที่มีจุดที่เขียนไม่ได้บางส่วน จะเรียกใช้ตัวนี้แทน commit และรับ commit message จากส่วนที่สำเร็จมาด้วย จากนั้นก็ตัดสินใจต่อว่าจะทำอะไรกับมันต่อไป ซึ่งในกรณีของตัวอย่างนี้ เราจะเขียนข้อความการ fail ลงใน textfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42eb16d9-b9d9-498b-b397-7f03744c9cc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.datasource import DataSourceStreamWriter, WriterCommitMessage\n",
    "\n",
    "class SimpleCommitMessage(WriterCommitMessage):\n",
    "   def __init__(self, partition_id: int, count: int):\n",
    "       self.partition_id = partition_id\n",
    "       self.count = count\n",
    "\n",
    "class FakeStreamWriter(DataSourceStreamWriter):\n",
    "   def __init__(self, options):\n",
    "       self.options = options\n",
    "       self.path = self.options.get(\"path\")\n",
    "       assert self.path is not None\n",
    "\n",
    "   def write(self, iterator):\n",
    "       \"\"\"\n",
    "       Writes the data and then returns the commit message for that partition. Library imports must be within the method.\n",
    "       \"\"\"\n",
    "       from pyspark import TaskContext\n",
    "       context = TaskContext.get()\n",
    "       partition_id = context.partitionId()\n",
    "       cnt = 0\n",
    "       for row in iterator:\n",
    "           cnt += 1\n",
    "       return SimpleCommitMessage(partition_id=partition_id, count=cnt)\n",
    "\n",
    "   def commit(self, messages, batchId) -> None:\n",
    "       \"\"\"\n",
    "       Receives a sequence of :class:`WriterCommitMessage` when all write tasks have succeeded, then decides what to do with it.\n",
    "       In this FakeStreamWriter, the metadata of the microbatch(number of rows and partitions) is written into a JSON file inside commit().\n",
    "       \"\"\"\n",
    "       status = dict(num_partitions=len(messages), rows=sum(m.count for m in messages))\n",
    "       with open(os.path.join(self.path, f\"{batchId}.json\"), \"a\") as file:\n",
    "           file.write(json.dumps(status) + \"\\n\")\n",
    "\n",
    "   def abort(self, messages, batchId) -> None:\n",
    "       \"\"\"\n",
    "       Receives a sequence of :class:`WriterCommitMessage` from successful tasks when some other tasks have failed, then decides what to do with it.\n",
    "       In this FakeStreamWriter, a failure message is written into a text file inside abort().\n",
    "       \"\"\"\n",
    "       with open(os.path.join(self.path, f\"{batchId}.txt\"), \"w\") as file:\n",
    "           file.write(f\"failed in batch {batchId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd7f2cb5-1a59-468b-be20-ca57773cc68a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Register and use the example data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "324bad73-2761-4a96-bfa4-1fa59511aecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "เดิมตาม docs จะเป็นโค้ดตามด้านล่าง แต่ในส่วนของ query นั้น เนื่องด้วย cluster แบบฟรีที่เราใช้ไม่ support micro-batch trigger สำหรับการ streaming ซึ่ง databricks แนะนำให้ใช้ `availableNow` หรือ `once` แทน และยังรวมไปถึงเรื่องการใช้ python กับ sink ที่ตัวฟรีอย่างเราไม่ support เช่นกัน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76c2fbae-91cd-42ff-b412-ade226946391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.dataSource.register(FakeStreamDataSource)\n",
    "# query = spark.readStream.format(\"fakestream\").load().writeStream.format(\"fake\").start(\"/output_path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c16890a-6f54-4af5-9ff3-fdb777dd1ff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ดังนั้น การแก้ไขปัญหา(เฉพาะหน้า) จากสิ่งที่เกิดขึ้น จึงต้องแก้ปัญหาด้วยการสร้าง folder ใหม่ในทุกครั้งที่เราจะ(ลอง)รันสิ่งนี้แทน โดยการใช้ uuid เข้ามาช่วย\n",
    "\n",
    "!! trigger = once ไม่ใช่ batch !!\n",
    "> trigger = once เป็น 1 ใน streaming query เช่นกัน\n",
    "\n",
    "และในการทำงานแบบ streaming นั้น **ต้องมี** checkpoint เสมอ เพื่อใช้ในการ recovery (แต่ตัวฟรีของเราไม่รองรับ T-T)\n",
    "\n",
    "นี่เป็นสาเหตุที่เราต้องแก้ไขเฉพาะหน้าด้วยวิธีข้างต้น เบื้องต้นตัวอย่างนี้จึงเกิดขึ้นเพื่อให้เข้าใจว่าถ้าจะ custom data sources แบบ streaming ควรจะเป็นประมาณไหน นั่นเองค่ะ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d25cf337-b599-4b21-b511-5b08be738cf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "checkpoint = f\"/Volumes/workspace/default/tutorial/_checkpoint/{uuid.uuid4()}\"\n",
    "# สร้าง path ของ checkpoint เพื่อไม่ให้ซ้ำ จะใช้ uuid มาช่วยสร้าง folder\n",
    "\n",
    "query = (\n",
    "    spark.readStream\n",
    "        .format(\"fakestream\") # เรียกใช้ data source ที่เราสร้างมา\n",
    "        .load()\n",
    "        .writeStream # เริ่มเขียน\n",
    "        .format(\"memory\") # เขียนลง memory แทน\n",
    "        .queryName(\"test_stream\") # ตั้งชื่อตารางปลายทาง\n",
    "        .option(\"checkpointLocation\", checkpoint) # ต้องมี checkpoint เสมอ โดยเราระบุ path ของ checkpoint ตาม path ข้างบน\n",
    "        .trigger(once=True) # รันรอบเดียวหยุด\n",
    "        .start()\n",
    ")\n",
    "\n",
    "# รอประมวลผลให้เสร็จ\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cb624bd-287f-40c3-9d34-a33a7bb5a1d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "และเพื่อที่เราจะดูว่าหน้าตาข้อมูลตัวอย่างเป็นเช่นไร คำตอบคือ ด้านล่างนี้ เราใช้ SQL มาช่วยในการ query ออกมาดู"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "123b31ec-a504-4aee-b149-d3d40d0ceee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM test_stream\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6666765633529300,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark-custom-data-sources",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
