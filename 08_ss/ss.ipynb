{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97db806a-be9d-4969-b72c-c35ec6863281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Run your first Structured Streaming workload\n",
    "[docs](https://docs.databricks.com/aws/en/structured-streaming/tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd22dcea-92a1-4bcb-b483-829df7b8c38e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Use Auto Loader to read streaming data from object storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd2c8e94-9ba0-4645-a4c0-2c4a826a7505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "load ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• json ‡∏î‡πâ‡∏ß‡∏¢ auto loader ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠ checkpoint path ‡πÄ‡∏õ‡πá‡∏ô volume ‡∏´‡∏£‡∏∑‡∏≠ path ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ vol, path ‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77918e19-5dcb-4a03-8770-a9fbe8665336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"/databricks-datasets/structured-streaming/events\"\n",
    "checkpoint_path = \"/Volumes/workspace/default/tutorial/sscheckpoint/\"\n",
    "\n",
    "raw_df = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", checkpoint_path)\n",
    "    .load(file_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2caf735f-6af4-4df2-b93f-041a345ff4ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Perform a streaming transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5024ed42-bf28-4e3d-9550-6fa1944bbf7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp\n",
    "\n",
    "transformed_df = (raw_df.select(\n",
    "    \"*\",\n",
    "    col(\"_metadata.file_path\").alias(\"source_file\"),\n",
    "    current_timestamp().alias(\"processing_time\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b806d1da-eaf8-44ea-82ab-5bd283ea66eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "‡∏Å‡∏≤‡∏£ display df ‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡πâ‡∏î‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô‡∏ô‡∏µ‡πâ ‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡πâ‡∏î display ‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á‡∏ñ‡πâ‡∏≤‡∏£‡∏±‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∞‡∏£‡∏±‡∏ô‡∏ú‡πà‡∏≤‡∏ô ‡πÅ‡∏ï‡πà‡∏£‡∏±‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö [output mode](https://docs.databricks.com/aws/en/structured-streaming/output-mode) ‡∏ã‡∏∂‡πà‡∏á display ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö complete mode ‡πÅ‡∏ï‡πà df ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏°‡∏µ aggregate ‡πÉ‡∏î‡πÜ (‡πÄ‡∏õ‡πá‡∏ô append mode) ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£ display ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
    "\n",
    "‡∏°‡∏µ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ aggregation ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô df, ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô checkpoint\n",
    "‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏•‡∏ö folder checkpoint ‡πÄ‡∏î‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏∞‡πÑ‡∏ß‡πâ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡∏£‡∏±‡∏ô cell ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏£‡∏≠‡∏ö ‡∏ï‡πà‡∏≠‡∏î‡πâ‡∏ß‡∏¢ cell ‡∏ó‡∏µ‡πà display ‡πÉ‡∏´‡∏°‡πà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f26d260-9171-4db7-a3bb-977b407d011b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"/Volumes/workspace/default/tutorial/sscheckpoint/\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fb5f1ac-12d6-46e6-917e-25737c662a7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  transformed_df,\n",
    "  checkpointLocation =\"/Volumes/workspace/default/tutorial/sscheckpoint/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9454e938-bce0-4aad-8008-1abcb3f7b7b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Perform an incremental batch write to Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e89fb39-94fc-4d4f-b2f1-4563b6593fb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "(‡∏Ç‡∏≠‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡∏¢‡πà‡∏≠ Structured Streaming ‡πÄ‡∏õ‡πá‡∏ô ss)\n",
    "\n",
    "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á Delta Lake ‡∏ã‡∏∂‡πà‡∏á‡πÉ‡∏ä‡πâ trigger ‡πÅ‡∏ö‡∏ö availableNow ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏Å‡∏≤‡∏£ trigger ‡∏ô‡∏µ‡πâ ‡∏à‡∏∞‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏ß ss process record ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏à‡∏≤‡∏Å source dataset ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢ shut down\n",
    "\n",
    "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏á‡∏ß‡∏•‡∏ß‡πà‡∏≤‡∏°‡∏±‡∏ô‡∏à‡∏∞‡πÑ‡∏õ‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£ streaming ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà\n",
    "\n",
    "> **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç** : ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏ checkpoint ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡πÄ‡∏î‡∏¥‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö streaming writer ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏≠‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏î‡πâ‡∏ß‡∏¢ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ checkpoint ‡∏à‡∏∞‡πÉ‡∏´‡πâ unique identity ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö stream, track record ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£ process ‡∏≠‡∏∞‡πÑ‡∏£‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏°‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á‡∏û‡∏ß‡∏Å state ‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏ô‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6bc253d-10ca-4cca-a6f2-77b1e983d742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ‡∏õ‡∏£‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠ target path, checkpoint path ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡πÇ‡∏î‡∏¢‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ ‡∏°‡∏±‡∏ô‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á path ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏á\n",
    "\n",
    "target_path = \"/Volumes/workspace/default/tutorial/ss-tutorial/\"\n",
    "checkpoint_path = \"/Volumes/workspace/default/tutorial/ss-checkpoint/\"\n",
    "\n",
    "transformed_df.writeStream.trigger(availableNow=True).option(\"checkpointLocation\", checkpoint_path).option(\"path\", target_path).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e9cfa97-6954-441d-ae02-fc2ef170bf9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "‡∏à‡∏≤‡∏Å docs tutorial ‡πÄ‡∏Ç‡∏≤‡∏ö‡∏≠‡∏Å‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡πà‡∏≤ ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏°‡∏µ record ‡πÉ‡∏´‡∏°‡πà‡πÜ ‡∏ñ‡πâ‡∏≤‡∏£‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏õ‡∏≠‡∏µ‡∏Å‡∏Å‡πá‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤\n",
    "\n",
    "‡πÅ‡∏•‡∏∞‡πÉ‡∏ô‡πÅ‡∏á‡πà‡∏Ç‡∏≠‡∏á production ‡∏à‡∏£‡∏¥‡∏á ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ß‡∏±‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏µ‡∏¢‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î‡∏Ñ‡∏≤‡∏î‡∏ù‡∏±‡∏ô ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å ss ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö auto ‡∏ï‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏õ‡∏¥‡∏î compute ‡πÑ‡∏õ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏â‡∏∞‡∏ô‡∏±‡πâ‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏•‡∏∑‡∏°‡∏´‡∏¢‡∏∏‡∏î query ‡∏Å‡πà‡∏≠‡∏ô‡∏î‡πâ‡∏ß‡∏¢!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11686802-e2f3-4a6c-a4fd-415d56b48563",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read data from Delta Lake, transform, and write to Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54332d37-c7a1-407d-b781-41ee23f80db2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "delta lake ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏±‡∏ö ss ‡∏ó‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö source & sink ‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô delta table ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏ô‡πâ‡∏≠‡∏¢ ‡∏°‡∏≤ join ‡∏Å‡∏±‡∏ö‡∏≠‡∏µ‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á(‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô delta ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô) ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô delta table ‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á\n",
    "\n",
    "(‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô path ‡∏Ç‡∏≠‡∏á checkpoint ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô path ‡πÉ‡∏´‡∏°‡πà, ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏î‡πâ‡∏ß‡∏¢ ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏Å‡πà‡∏≤‡∏Ç‡∏≠‡∏á customers ‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á LDP ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å customers, customers_history_agg ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô pipeline ‡πÉ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ ‡∏ã‡∏∂‡πà‡∏á‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ï‡∏≤‡∏£‡∏≤‡∏á 2 ‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏µ id ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‡πÅ‡∏•‡∏∞‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ö‡∏ô databricks ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏ö‡∏ö [delta table ‡πÇ‡∏î‡∏¢ default](https://docs.databricks.com/aws/en/delta/#getting-started-with-delta-lake) ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏•‡∏¢‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏≠‡∏∞‡πÑ‡∏£)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd935b6d-6ca7-4ea2-9012-1919b4d7787f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.readStream\n",
    "    .table(\"customers\")\n",
    "    .join(spark.read.table(\"customers_history_agg\"), on=\"id\", how=\"left\")\n",
    "    .writeStream\n",
    "    .trigger(availableNow=True)\n",
    "    .option(\"checkpointLocation\", \"/Volumes/workspace/default/tutorial/ss-checkpoint/delta\")\n",
    "    .toTable(\"testread_dt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "083ff7e0-b05e-4106-9afd-65ad3b9fcaf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read data from Kafka, transform, and write to Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02afd945-01af-46a0-821d-d89cbe29a758",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Apache Kafka ‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (messaging buses) ‡∏≠‡∏∑‡πà‡∏ô‡πÜ  ‡πÉ‡∏´‡πâ latency ‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö dataset ‡∏Ç‡∏ô‡∏≤‡∏î‡∏°‡∏´‡∏∂‡∏°‡∏≤ ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ Databricks ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà ingested ‡∏à‡∏≤‡∏Å Kafka ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏¢‡∏±‡∏á Kafka ‡πÑ‡∏î‡πâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43f7aa48-2771-411f-be8e-124665f78b32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á code ‡∏ó‡∏µ‡πà databricks ‡πÉ‡∏´‡πâ‡∏°‡∏≤\n",
    "```\n",
    "\n",
    "(spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"<server:ip>\")\n",
    "    .option(\"subscribe\", \"<topic>\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    "    .join(spark.read.table(\"<table-name>\"), on=\"<id>\", how=\"left\")\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"<server:ip>\")\n",
    "    .option(\"topic\", \"<topic>\")\n",
    "    .option(\"checkpointLocation\", \"<checkpoint-path>\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19ef46b8-2bd9-47a3-a449-0532a8efdb41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡πâ‡∏î‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô‡∏ô‡∏µ‡πâ ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô ‡∏à‡∏≤‡∏Å kafka ‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏ß‡πà‡∏≤ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏°‡∏±‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡πÄ‡∏•‡∏¢‡πÑ‡∏õ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏™‡∏°‡∏±‡∏Ñ‡∏£ [Oracle cloud (free tier)](https://www.oracle.com/asean/cloud/free) ‡∏°‡∏≤ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ kafka ‡∏î‡∏π‡∏à‡∏£‡∏¥‡∏á‡πÜ‡∏ö‡∏ô databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2722127b-c523-47e7-a4e9-f6f52b686e7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏°‡∏≤ 1 ‡∏ï‡∏≤‡∏£‡∏≤‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ join ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å kafka\n",
    "\n",
    "CREATE TABLE dim_users (\n",
    "  id STRING,\n",
    "  name STRING\n",
    ");\n",
    "\n",
    "INSERT INTO dim_users VALUES\n",
    "('1', 'Alice'),\n",
    "('2', 'Bob');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "535ab7f8-6d7f-46d1-8436-8ce4043e254a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏≤ setup ‡∏°‡∏´‡∏≤‡∏®‡∏≤‡∏•‡∏ö‡∏ô cloud ‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏•‡∏≠‡∏á‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏•‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏°‡∏≤‡∏à‡∏≤‡∏Å kafka ‡∏à‡∏£‡∏¥‡∏á‡πÜ‡∏î‡∏π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c5a381-c256-4a40-9381-b86b9b7ec8d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "checkpoint_path = f'/Volumes/workspace/default/tutorial/kafka_ckpt/{uuid.uuid4()}'\n",
    "(\n",
    "    spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", \"168.138.166.160:29092\")\n",
    "        .option(\"subscribe\", \"test-topic\")\n",
    "        .option(\"startingOffsets\", \"earliest\")\n",
    "        .load()\n",
    "        .selectExpr(\n",
    "            \"CAST(key AS STRING) AS id\",\n",
    "            \"CAST(value AS STRING) AS message\"\n",
    "        )\n",
    "        .join(\n",
    "            spark.read.table(\"dim_users\"),\n",
    "            on=\"id\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "        .writeStream\n",
    "        .format(\"memory\")\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\n",
    "        .option(\"queryName\", \"kafka_memory\")\n",
    "        .trigger(availableNow=True)\n",
    "        .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7823d071-a21f-4006-9fd0-505928a0b6c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏û‡∏¥‡∏°‡∏û‡πå‡∏™‡πà‡∏á‡∏à‡∏≤‡∏Å console ‡∏Ç‡∏≠‡∏á host \n",
    "_**‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ó‡∏µ‡πà query ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà**_\n",
    "> start query -> ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ó‡∏µ‡πà query ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà -> ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ö‡∏ô console ‡∏Ç‡∏≠‡∏á host -> query ‡∏£‡∏±‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à -> ‡∏•‡∏≠‡∏á‡πÄ‡∏ä‡πá‡∏Ñ‡∏î‡πâ‡∏ß‡∏¢ sql ‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á\n",
    "\n",
    "(‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏á free edition ‡∏ó‡∏µ‡πà trigger ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÄ‡∏•‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡∏ö ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ß‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ‡∏®‡∏≤‡∏à ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏î‡∏π)\n",
    "\n",
    "‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡πÅ‡∏Ç‡πà‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏ß‡∏•‡∏≤ ‡πÉ‡∏ä‡πâ `.option(\"startingOffsets\", \"latest\")` ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô `earliest` ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏±‡∏ô‡∏à‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô kafka ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏° query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "252cf379-9f02-4bd4-b3fc-04aa46617817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM kafka_memory;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc0e5867-52f7-4c63-8d73-bc6826bb7593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17e1d09b-0861-4fa4-8676-3b788d469a17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Structured Streaming patterns on Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79272dbf-bc64-4ef4-bc75-b355bbb0ec32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "[docs](https://docs.databricks.com/aws/en/structured-streaming/examples)\n",
    "\n",
    "‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö ss ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏á free edition ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ serverless compute ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏•‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡πÄ‡∏™‡∏£‡∏¥‡∏°‡πÉ‡∏ô compute ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏≤‡∏¢‡πÜ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô‡∏ô‡∏µ‡πâ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏ï‡πà‡∏≤‡∏á‡πÜ (‡∏û‡∏ß‡∏Å lib, connector, etc‡∏Ø) ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏£‡∏±‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÜ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53ff1b77-537e-49cd-92a3-276275c6a2c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write to Cassandra as a sink for Structured Streaming in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd8149c7-8334-400f-b289-f4ec36b79e14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ss ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö cassandra ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ [Spark Cassandra Connector](https://github.com/apache/cassandra-spark-connector) ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á RDD, API, DF ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö streaming ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á\n",
    "\n",
    "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á‡∏Ñ‡∏∑‡∏≠ show ‡∏ß‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡∏à‡∏∞‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö host ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà 1 host ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ‡πÉ‡∏ô cassandra db ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡πÑ‡∏´‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á config ‡∏û‡∏ß‡∏Å checkpoint location, ‡∏ä‡∏∑‡πà‡∏≠ keyspace, ‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≤‡∏£‡∏≤‡∏á\n",
    "\n",
    "(‡πÇ‡∏Ñ‡πâ‡∏î‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏•‡∏≠‡∏á‡∏£‡∏±‡∏ô‡∏à‡∏£‡∏¥‡∏á!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc5832f7-622f-484c-9803-4b6c4e73c4d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.cassandra.connection.host\", \"host1,host2\")\n",
    "\n",
    "df.writeStream \\\n",
    "  .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "  .option(\"keyspace\", \"keyspace_name\") \\\n",
    "  .option(\"table\", \"table_name\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c86bee38-7d8a-425c-8785-241c99a8425d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write to Azure Synapse Analytics using `foreachBatch()` in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "509c43f1-bfe2-4b89-b2da-032356e0d243",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "‡∏ô‡∏≥ [foreachBatch()](https://docs.databricks.com/aws/en/structured-streaming/foreach) ‡∏°‡∏≤‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ú‡∏•‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ aggregation ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á\n",
    "\n",
    "‡∏õ‡∏Å‡∏ï‡∏¥ pattern ‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏ô‡∏µ‡πâ `streamingDF.writeStream.foreachBatch(...)` ‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏±‡∏ô‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ batch function ‡∏Å‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡πÜ micro-batch ‡πÉ‡∏ô streaming query ‡πÑ‡∏î‡πâ ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ö 2 parameters ‡∏Ñ‡∏∑‡∏≠\n",
    "- df ‡∏ó‡∏µ‡πà‡∏°‡∏µ output ‡πÄ‡∏õ‡πá‡∏ô micro-batch\n",
    "- unique id ‡∏Ç‡∏≠‡∏á micro-batch\n",
    "\n",
    "‡∏õ‡∏•. ‡∏à‡∏£‡∏¥‡∏á‡πÜ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ ‡∏´‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡∏°‡∏µ account ‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ñ‡∏∂‡∏á‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏≠‡∏á‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏™‡∏°‡∏±‡∏Ñ‡∏£ account student ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠ Azure Synapse Analytics ‡∏û‡∏ö‡∏ß‡πà‡∏≤‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå Student ‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ñ‡∏∂‡∏á‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ ‡πÄ‡∏•‡∏¢‡∏•‡∏≠‡∏á‡πÄ‡∏•‡πà‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÜ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b86fa41-2218-4e9a-b845-29aa41737dac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "def writeToSQLWarehouse(df, epochId):\n",
    "  df.write \\\n",
    "    .format(\"com.databricks.spark.sqldw\") \\\n",
    "    .mode('overwrite') \\\n",
    "    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n",
    "    .option(\"forward_spark_azure_storage_credentials\", \"true\") \\\n",
    "    .option(\"dbtable\", \"my_table_in_dw_copy\") \\\n",
    "    .option(\"tempdir\", \"wasbs://<your-container-name>@<your-storage-account-name>.blob.core.windows.net/<your-directory-name>\") \\\n",
    "    .save()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n",
    "\n",
    "checkpoint_path = \"/path/to/checkpoint\" # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏≤\n",
    "query = (\n",
    "  spark.readStream.format(\"rate\").load()\n",
    "    .selectExpr(\"value % 10 as key\")\n",
    "    .groupBy(\"key\")\n",
    "    .count()\n",
    "    .toDF(\"key\", \"count\")\n",
    "    .writeStream\n",
    "    .foreachBatch(writeToSQLWarehouse)\n",
    "    .outputMode(\"update\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path) # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏≤\n",
    "    .start()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47d10c07-9bda-402a-9b56-7ea85951dbeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write to Amazon DynamoDB using foreach() in Scala and Python _(in this case we use Python only ka)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38280156-d55c-4ba4-a344-00db35dde5ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏™‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ foreach() ‡∏ã‡∏∂‡πà‡∏á‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô output ‡∏à‡∏≤‡∏Å streaming query ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô‡∏Å‡πá‡πÑ‡∏î‡πâ\n",
    "\n",
    "‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏ß‡πà‡∏≤ ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏°‡∏µ Amazon DynamoDB (‡∏•‡∏≠‡∏á‡∏î‡∏π‡∏Ç‡∏≠‡∏á aws ‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏û‡∏ö‡∏ß‡πà‡∏≤‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡∏ü‡∏£‡∏µ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ 6 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ôüò≠ ‡∏≠‡∏µ‡∏Å‡∏ó‡∏±‡πâ‡∏á ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏à‡∏∞‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÑ‡∏õ‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢ ‡πÄ‡∏•‡∏¢‡πÇ‡∏î‡∏ô suspended account ‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß) ‡πÄ‡∏•‡∏¢‡∏à‡∏∞‡πÄ‡∏•‡πà‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡πà‡∏≤‡∏ß‡πÜ‡πÅ‡∏ó‡∏ô‡∏Ñ‡πà‡∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acb91771-200e-4236-9379-73218922ff3d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 1"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1 : define function ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á DynamoDB Table\n",
    "table_name = \"PythonForeachTest\"\n",
    "\n",
    "def get_dynamodb():\n",
    "  import boto3\n",
    "\n",
    "  access_key = \"<access key>\"\n",
    "  secret_key = \"<secret key>\"\n",
    "  region = \"<region name>\"\n",
    "  return boto3.resource('dynamodb',\n",
    "                 aws_access_key_id=access_key,\n",
    "                 aws_secret_access_key=secret_key,\n",
    "                 region_name=region)\n",
    "\n",
    "def createTableIfNotExists():\n",
    "    '''\n",
    "    Create a DynamoDB table if it does not exist.\n",
    "    This must be run on the Spark driver, and not inside foreach.\n",
    "    '''\n",
    "    dynamodb = get_dynamodb()\n",
    "\n",
    "    existing_tables = dynamodb.meta.client.list_tables()['TableNames']\n",
    "    if table_name not in existing_tables:\n",
    "      print(\"Creating table %s\" % table_name)\n",
    "      table = dynamodb.create_table(\n",
    "          TableName=table_name,\n",
    "          KeySchema=[ { 'AttributeName': 'key', 'KeyType': 'HASH' } ],\n",
    "          AttributeDefinitions=[ { 'AttributeName': 'key', 'AttributeType': 'S' } ],\n",
    "          ProvisionedThroughput = { 'ReadCapacityUnits': 5, 'WriteCapacityUnits': 5 }\n",
    "      )\n",
    "\n",
    "      print(\"Waiting for table to be ready\")\n",
    "\n",
    "table.meta.client.get_waiter('table_exists').wait(TableName=table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab832c78-c495-4799-84b5-687a8247662e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "step 2 : define class & method ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÉ‡∏ô DynamoDB ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å foreach ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡∏°‡∏µ 2 ‡∏ß‡∏¥‡∏ò‡∏µ ‡∏Ñ‡∏∑‡∏≠‡πÉ‡∏ä‡πâ function ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ class\n",
    "- function : ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å function ‡∏ï‡πà‡∏≠ 1 row ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏ö simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "888baad3-c625-4c24-9af9-9d0bc86d4514",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 2 Choice 1"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2 : Use a function\n",
    "def sendToDynamoDB_simple(row):\n",
    "  '''\n",
    "  Function to send a row to DynamoDB.\n",
    "  When used with `foreach`, this method is going to be called in the executor\n",
    "  with the generated output rows.\n",
    "  '''\n",
    "  # Create client object in the executor,\n",
    "  # do not use client objects created in the driver\n",
    "  dynamodb = get_dynamodb()\n",
    "\n",
    "  dynamodb.Table(table_name).put_item(\n",
    "      Item = { 'key': str(row['key']), 'count': row['count'] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70427f3a-935a-43d1-80b7-33bb5d6064b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- class (‡∏°‡∏µ method : open, process, close) : ‡∏à‡∏∞‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏¥‡∏î‡∏ô‡∏∂‡∏á ‡πÅ‡∏ï‡πà‡∏ß‡πà‡∏≤‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÜ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤  ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢ row ‡∏ï‡πà‡∏≠ 1 connection ‡πÅ‡∏•‡∏∞‡∏û‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏Å connection ‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a480a4a6-8cde-43b0-95f6-1cbbae7b1edc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 2 Choice 2"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2 : Use a class\n",
    "class SendToDynamoDB_ForeachWriter:\n",
    "  '''\n",
    "  Class to send a set of rows to DynamoDB.\n",
    "  When used with `foreach`, copies of this class is going to be used to write\n",
    "  multiple rows in the executor. See the python docs for `DataStreamWriter.foreach`\n",
    "  for more details.\n",
    "  '''\n",
    "\n",
    "  def open(self, partition_id, epoch_id):\n",
    "    # This is called first when preparing to send multiple rows.\n",
    "    # Put all the initialization code inside open() so that a fresh\n",
    "    # copy of this class is initialized in the executor where open()\n",
    "    # will be called.\n",
    "    self.dynamodb = get_dynamodb()\n",
    "    return True\n",
    "\n",
    "  def process(self, row):\n",
    "    # This is called for each row after open() has been called.\n",
    "    # This implementation sends one row at a time.\n",
    "    # For further enhancements, contact the Spark+DynamoDB connector\n",
    "    # team: https://github.com/audienceproject/spark-dynamodb\n",
    "    self.dynamodb.Table(table_name).put_item(\n",
    "        Item = { 'key': str(row['key']), 'count': row['count'] })\n",
    "\n",
    "  def close(self, err):\n",
    "    # This is called after all the rows have been processed.\n",
    "    if err:\n",
    "      raise err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8054f7f2-4c4e-447f-ac38-79f9916fd88d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 3 : ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ foreach ‡πÉ‡∏ô streaming query ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ object/function ‡∏ó‡∏µ‡πà define ‡πÑ‡∏õ\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n",
    "\n",
    "query = (\n",
    "  spark.readStream.format(\"rate\").load()\n",
    "    .selectExpr(\"value % 10 as key\")\n",
    "    .groupBy(\"key\")\n",
    "    .count()\n",
    "    .toDF(\"key\", \"count\")\n",
    "    .writeStream\n",
    "    .foreach(SendToDynamoDB_ForeachWriter())\n",
    "    #.foreach(sendToDynamoDB_simple)  // alternative, use one or the other\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6876a63e-2557-472c-a5b9-994c0e9696d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Stream-Stream joins\n",
    "\n",
    "‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏≠‡∏µ‡∏Å [Notebook](url)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8456703040381402,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ss",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
