{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97db806a-be9d-4969-b72c-c35ec6863281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Run your first Structured Streaming workload\n",
    "[docs](https://docs.databricks.com/aws/en/structured-streaming/tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd22dcea-92a1-4bcb-b483-829df7b8c38e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Use Auto Loader to read streaming data from object storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77918e19-5dcb-4a03-8770-a9fbe8665336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"/databricks-datasets/structured-streaming/events\"\n",
    "checkpoint_path = \"/tmp/ss-tutorial/_checkpoint\"\n",
    "\n",
    "raw_df = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", checkpoint_path)\n",
    "    .load(file_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2caf735f-6af4-4df2-b93f-041a345ff4ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Perform a streaming transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5024ed42-bf28-4e3d-9550-6fa1944bbf7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp\n",
    "\n",
    "transformed_df = (raw_df.select(\n",
    "    \"*\",\n",
    "    col(\"_metadata.file_path\").alias(\"source_file\"),\n",
    "    current_timestamp().alias(\"processing_time\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9454e938-bce0-4aad-8008-1abcb3f7b7b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Perform an incremental batch write to Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6bc253d-10ca-4cca-a6f2-77b1e983d742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target_path = \"/tmp/ss-tutorial/\"\n",
    "checkpoint_path = \"/tmp/ss-tutorial/_checkpoint\"\n",
    "\n",
    "transformed_df.writeStream\n",
    "    .trigger(availableNow=True)\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .option(\"path\", target_path)\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11686802-e2f3-4a6c-a4fd-415d56b48563",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read data from Delta Lake, transform, and write to Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd935b6d-6ca7-4ea2-9012-1919b4d7787f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.readStream\n",
    "    .table(\"<table-name1>\")\n",
    "    .join(spark.read.table(\"<table-name2>\"), on=\"<id>\", how=\"left\")\n",
    "    .writeStream\n",
    "    .trigger(availableNow=True)\n",
    "    .option(\"checkpointLocation\", \"<checkpoint-path>\")\n",
    "    .toTable(\"<table-name3>\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "083ff7e0-b05e-4106-9afd-65ad3b9fcaf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read data from Kafka, transform, and write to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98f2bb21-655d-404f-b132-abe5cd4a46cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"<server:ip>\")\n",
    "    .option(\"subscribe\", \"<topic>\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    "    .join(spark.read.table(\"<table-name>\"), on=\"<id>\", how=\"left\")\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"<server:ip>\")\n",
    "    .option(\"topic\", \"<topic>\")\n",
    "    .option(\"checkpointLocation\", \"<checkpoint-path>\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc0e5867-52f7-4c63-8d73-bc6826bb7593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17e1d09b-0861-4fa4-8676-3b788d469a17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Structured Streaming patterns on Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79272dbf-bc64-4ef4-bc75-b355bbb0ec32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53ff1b77-537e-49cd-92a3-276275c6a2c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write to Cassandra as a sink for Structured Streaming in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc5832f7-622f-484c-9803-4b6c4e73c4d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.cassandra.connection.host\", \"host1,host2\")\n",
    "\n",
    "df.writeStream \\\n",
    "  .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "  .option(\"keyspace\", \"keyspace_name\") \\\n",
    "  .option(\"table\", \"table_name\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c86bee38-7d8a-425c-8785-241c99a8425d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write to Azure Synapse Analytics using foreachBatch() in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b86fa41-2218-4e9a-b845-29aa41737dac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "def writeToSQLWarehouse(df, epochId):\n",
    "  df.write \\\n",
    "    .format(\"com.databricks.spark.sqldw\") \\\n",
    "    .mode('overwrite') \\\n",
    "    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n",
    "    .option(\"forward_spark_azure_storage_credentials\", \"true\") \\\n",
    "    .option(\"dbtable\", \"my_table_in_dw_copy\") \\\n",
    "    .option(\"tempdir\", \"wasbs://<your-container-name>@<your-storage-account-name>.blob.core.windows.net/<your-directory-name>\") \\\n",
    "    .save()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n",
    "\n",
    "query = (\n",
    "  spark.readStream.format(\"rate\").load()\n",
    "    .selectExpr(\"value % 10 as key\")\n",
    "    .groupBy(\"key\")\n",
    "    .count()\n",
    "    .toDF(\"key\", \"count\")\n",
    "    .writeStream\n",
    "    .foreachBatch(writeToSQLWarehouse)\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47d10c07-9bda-402a-9b56-7ea85951dbeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write to Amazon DynamoDB using foreach() in Scala and Python (in this case we use Python only ka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acb91771-200e-4236-9379-73218922ff3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "table_name = \"PythonForeachTest\"\n",
    "\n",
    "def get_dynamodb():\n",
    "  import boto3\n",
    "\n",
    "  access_key = \"<access key>\"\n",
    "  secret_key = \"<secret key>\"\n",
    "  region = \"<region name>\"\n",
    "  return boto3.resource('dynamodb',\n",
    "                 aws_access_key_id=access_key,\n",
    "                 aws_secret_access_key=secret_key,\n",
    "                 region_name=region)\n",
    "\n",
    "def createTableIfNotExists():\n",
    "    '''\n",
    "    Create a DynamoDB table if it does not exist.\n",
    "    This must be run on the Spark driver, and not inside foreach.\n",
    "    '''\n",
    "    dynamodb = get_dynamodb()\n",
    "\n",
    "    existing_tables = dynamodb.meta.client.list_tables()['TableNames']\n",
    "    if table_name not in existing_tables:\n",
    "      print(\"Creating table %s\" % table_name)\n",
    "      table = dynamodb.create_table(\n",
    "          TableName=table_name,\n",
    "          KeySchema=[ { 'AttributeName': 'key', 'KeyType': 'HASH' } ],\n",
    "          AttributeDefinitions=[ { 'AttributeName': 'key', 'AttributeType': 'S' } ],\n",
    "          ProvisionedThroughput = { 'ReadCapacityUnits': 5, 'WriteCapacityUnits': 5 }\n",
    "      )\n",
    "\n",
    "      print(\"Waiting for table to be ready\")\n",
    "\n",
    "table.meta.client.get_waiter('table_exists').wait(TableName=table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "888baad3-c625-4c24-9af9-9d0bc86d4514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2 : Use a function\n",
    "def sendToDynamoDB_simple(row):\n",
    "  '''\n",
    "  Function to send a row to DynamoDB.\n",
    "  When used with `foreach`, this method is going to be called in the executor\n",
    "  with the generated output rows.\n",
    "  '''\n",
    "  # Create client object in the executor,\n",
    "  # do not use client objects created in the driver\n",
    "  dynamodb = get_dynamodb()\n",
    "\n",
    "  dynamodb.Table(table_name).put_item(\n",
    "      Item = { 'key': str(row['key']), 'count': row['count'] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a480a4a6-8cde-43b0-95f6-1cbbae7b1edc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2 : Use a class\n",
    "class SendToDynamoDB_ForeachWriter:\n",
    "  '''\n",
    "  Class to send a set of rows to DynamoDB.\n",
    "  When used with `foreach`, copies of this class is going to be used to write\n",
    "  multiple rows in the executor. See the python docs for `DataStreamWriter.foreach`\n",
    "  for more details.\n",
    "  '''\n",
    "\n",
    "  def open(self, partition_id, epoch_id):\n",
    "    # This is called first when preparing to send multiple rows.\n",
    "    # Put all the initialization code inside open() so that a fresh\n",
    "    # copy of this class is initialized in the executor where open()\n",
    "    # will be called.\n",
    "    self.dynamodb = get_dynamodb()\n",
    "    return True\n",
    "\n",
    "  def process(self, row):\n",
    "    # This is called for each row after open() has been called.\n",
    "    # This implementation sends one row at a time.\n",
    "    # For further enhancements, contact the Spark+DynamoDB connector\n",
    "    # team: https://github.com/audienceproject/spark-dynamodb\n",
    "    self.dynamodb.Table(table_name).put_item(\n",
    "        Item = { 'key': str(row['key']), 'count': row['count'] })\n",
    "\n",
    "  def close(self, err):\n",
    "    # This is called after all the rows have been processed.\n",
    "    if err:\n",
    "      raise err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8054f7f2-4c4e-447f-ac38-79f9916fd88d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n",
    "\n",
    "query = (\n",
    "  spark.readStream.format(\"rate\").load()\n",
    "    .selectExpr(\"value % 10 as key\")\n",
    "    .groupBy(\"key\")\n",
    "    .count()\n",
    "    .toDF(\"key\", \"count\")\n",
    "    .writeStream\n",
    "    .foreach(SendToDynamoDB_ForeachWriter())\n",
    "    #.foreach(sendToDynamoDB_simple)  // alternative, use one or the other\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6876a63e-2557-472c-a5b9-994c0e9696d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Stream-Stream joins\n",
    "\n",
    "จะอยู่ในอีก [Notebook](url)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ss",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
